{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with them through their APIs.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a git pull and merge your changes as needed</a>. Check out the GitHub guide for instructions. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys - OPTIONAL!\n",
    "\n",
    "We're now going to try asking a bunch of models some questions!\n",
    "\n",
    "This is totally optional. If you have keys to Anthropic, Gemini or others, then you can add them in.\n",
    "\n",
    "If you'd rather not spend the extra, then just watch me do it!\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api   \n",
    "For DeepSeek, visit https://platform.deepseek.com/  \n",
    "For Groq, visit https://console.groq.com/  \n",
    "For Grok, visit https://console.x.ai/  \n",
    "\n",
    "\n",
    "You can also use OpenRouter as your one-stop-shop for many of these! OpenRouter is \"the unified interface for LLMs\":\n",
    "\n",
    "For OpenRouter, visit https://openrouter.ai/  \n",
    "\n",
    "\n",
    "With each of the above, you typically have to navigate to:\n",
    "1. Their billing page to add the minimum top-up (except Gemini, Groq, Google, OpenRouter may have free tiers)\n",
    "2. Their API key page to collect your API key\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "GROQ_API_KEY=xxxx\n",
    "GROK_API_KEY=xxxx\n",
    "OPENROUTER_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Any time you change your .env file</h2>\n",
    "            <span style=\"color:#900;\">Remember to Save it! And also rerun load_dotenv(override=True)<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:06:03.448559Z",
     "iopub.status.busy": "2025-10-16T23:06:03.447992Z",
     "iopub.status.idle": "2025-10-16T23:06:03.900214Z",
     "shell.execute_reply": "2025-10-16T23:06:03.899930Z",
     "shell.execute_reply.started": "2025-10-16T23:06:03.448516Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import google.generativeai as genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0abffac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:06:05.044359Z",
     "iopub.status.busy": "2025-10-16T23:06:05.043588Z",
     "iopub.status.idle": "2025-10-16T23:06:05.058422Z",
     "shell.execute_reply": "2025-10-16T23:06:05.057905Z",
     "shell.execute_reply.started": "2025-10-16T23:06:05.044304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key not set (and this is optional)\n",
      "Groq API Key not set (and this is optional)\n",
      "Grok API Key not set (and this is optional)\n",
      "OpenRouter API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "985a859a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:06:08.483984Z",
     "iopub.status.busy": "2025-10-16T23:06:08.483307Z",
     "iopub.status.idle": "2025-10-16T23:06:08.599319Z",
     "shell.execute_reply": "2025-10-16T23:06:08.599072Z",
     "shell.execute_reply.started": "2025-10-16T23:06:08.483932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16813180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:06:11.131072Z",
     "iopub.status.busy": "2025-10-16T23:06:11.130636Z",
     "iopub.status.idle": "2025-10-16T23:06:11.134671Z",
     "shell.execute_reply": "2025-10-16T23:06:11.134085Z",
     "shell.execute_reply.started": "2025-10-16T23:06:11.131043Z"
    }
   },
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"system\", \"content\": \"you are a sarchastic and somewhat inappropriate comedian who liked to tell insult jokes\" },\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e92304",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ea76a",
   "metadata": {},
   "source": [
    "## Training vs Inference time scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe9e11c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:06:13.137047Z",
     "iopub.status.busy": "2025-10-16T23:06:13.136617Z",
     "iopub.status.idle": "2025-10-16T23:06:13.140522Z",
     "shell.execute_reply": "2025-10-16T23:06:13.139833Z",
     "shell.execute_reply.started": "2025-10-16T23:06:13.137019Z"
    }
   },
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a887eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f854d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"low\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45fc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca713a5c",
   "metadata": {},
   "source": [
    "## Testing out the best models on the planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1e825b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:35:12.007986Z",
     "iopub.status.busy": "2025-10-16T23:35:12.007567Z",
     "iopub.status.idle": "2025-10-16T23:35:12.011469Z",
     "shell.execute_reply": "2025-10-16T23:35:12.010945Z",
     "shell.execute_reply.started": "2025-10-16T23:35:12.007960Z"
    }
   },
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=hard_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693ac0d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1dc5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-2.5-pro\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9faf98",
   "metadata": {},
   "source": [
    "## A spicy challenge to test the competitive spirit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc1824ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:52:41.890728Z",
     "iopub.status.busy": "2025-10-16T22:52:41.890294Z",
     "iopub.status.idle": "2025-10-16T22:52:41.894323Z",
     "shell.execute_reply": "2025-10-16T22:52:41.893829Z",
     "shell.execute_reply.started": "2025-10-16T22:52:41.890701Z"
    }
   },
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" — if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" — if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {\"role\": \"user\", \"content\": dilemma_prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09807f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=dilemma, reasoning_effort=\"medium\")\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "\n",
    "# response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=dilemma)\n",
    "# display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = deepseek.chat.completions.create(model=\"deepseek-reasoner\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = grok.chat.completions.create(model=\"grok-4\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162752e9",
   "metadata": {},
   "source": [
    "## Going local\n",
    "\n",
    "Just use the OpenAI library pointed to localhost:11434/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"http://localhost:11434/\").content\n",
    "\n",
    "# If not running, run ollama serve at a command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e97263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do this if you have a large machine - at least 16GB RAM\n",
    "\n",
    "!ollama pull gpt-oss:20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bfc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5527a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"gpt-oss:20b\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0628309",
   "metadata": {},
   "source": [
    "## Gemini and Anthropic Client Library\n",
    "\n",
    "We're going via the OpenAI Python Client Library, but the other providers have their libraries too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:50:03.407798Z",
     "iopub.status.busy": "2025-10-16T22:50:03.407364Z",
     "iopub.status.idle": "2025-10-16T22:50:04.239404Z",
     "shell.execute_reply": "2025-10-16T22:50:04.238706Z",
     "shell.execute_reply.started": "2025-10-16T22:50:03.407771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you for asking! I'm a large language model, so I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you.\n",
      "\n",
      "How are *you* doing today? What can I help you with?\n"
     ]
    }
   ],
   "source": [
    "# # from google import genai\n",
    "# import google.generativeai as genai\n",
    "# client = genai.Client()\n",
    "# response = client.models.generate_content( \n",
    "#     model=\"gemini-2.5-flash-lite\", contents=\"Describe the color Blue to someone who's never been able to see in 1 sentence\"\n",
    "# )\n",
    "# print(response.text)\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure with your API key\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "# Create a model instance (not Client)\n",
    "model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
    "\n",
    "# Generate content\n",
    "response = model.generate_content(\"Hello, how are you?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df7b6c63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:51:14.613287Z",
     "iopub.status.busy": "2025-10-16T22:51:14.612856Z",
     "iopub.status.idle": "2025-10-16T22:51:17.790348Z",
     "shell.execute_reply": "2025-10-16T22:51:17.789831Z",
     "shell.execute_reply.started": "2025-10-16T22:51:14.613259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue is the color that feels like cool water on your skin, sounds like calm ocean waves or a soft whistle, and carries the peaceful quietness of early morning air.\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Describe the color Blue to someone who's never been able to see in 1 sentence\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9d0eb",
   "metadata": {},
   "source": [
    "## Routers and Abtraction Layers\n",
    "\n",
    "Starting with the wonderful OpenRouter.ai - it can connect to all the models above!\n",
    "\n",
    "Visit openrouter.ai and browse the models.\n",
    "\n",
    "Here's one we haven't seen yet: GLM 4.5 from Chinese startup z.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openrouter.chat.completions.create(model=\"z-ai/glm-4.5\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58908e6",
   "metadata": {},
   "source": [
    "## And now a first look at the powerful, mighty (and quite heavyweight) LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02e145ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:58:12.277449Z",
     "iopub.status.busy": "2025-10-16T22:58:12.276999Z",
     "iopub.status.idle": "2025-10-16T22:58:34.006192Z",
     "shell.execute_reply": "2025-10-16T22:58:34.005346Z",
     "shell.execute_reply.started": "2025-10-16T22:58:12.277421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Training to be an LLM engineer, huh? Nice — you’re now 70% Stack Overflow, 20% cold coffee and 10% optimistic hallucinations. Keep at it; as an “expert” you’ll be able to make a 200B-parameter model confidently lie for you while your code quietly refuses to run."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "response = llm.invoke(tell_a_joke)\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d49785",
   "metadata": {},
   "source": [
    "## Finally - my personal fave - the wonderfully lightweight LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63e42515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:35:21.624729Z",
     "iopub.status.busy": "2025-10-16T23:35:21.623944Z",
     "iopub.status.idle": "2025-10-16T23:35:51.739286Z",
     "shell.execute_reply": "2025-10-16T23:35:51.738861Z",
     "shell.execute_reply.started": "2025-10-16T23:35:21.624672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's carefully analyze the puzzle:\n",
       "\n",
       "- Two volumes of Pushkin stand **side by side** on a bookshelf: **the first and the second**.\n",
       "- Each volume:\n",
       "    - Pages: **2 cm**\n",
       "    - Each cover: **2 mm** (= 0.2 cm)\n",
       "\n",
       "- The worm gnawed from the **first page of the first volume** to the **last page of the second volume**, in a **straight line perpendicular to the pages**.\n",
       "\n",
       "Let’s picture the books:\n",
       "\n",
       "When two books are placed on a bookshelf, usually the order is such that the **spine** (where the title is) faces outward, and the page edges touch each other on the inner side.\n",
       "\n",
       "- **First page of the first volume** is on the rightmost edge of the first book (closest to the second volume)\n",
       "- **Last page of the second volume** is on the leftmost edge of the second book (closest to the first volume)\n",
       "\n",
       "Visually:\n",
       "\n",
       "```\n",
       "[left] | [back cover of 1st vol] [pages of 1st vol] [front cover of 1st vol] [front cover of 2nd vol] [pages of 2nd vol] [back cover of 2nd vol] | [right]\n",
       "----------------|---------------------------|\n",
       " Books         are adjacent at        these covers\n",
       "```\n",
       "\n",
       "So the **first page of Vol 1** is just inside the **front cover** of volume 1 (which is next to the front cover of volume 2), and the **last page of Vol 2** is just inside the **back cover** of volume 2.\n",
       "\n",
       "But notice! **The \"first page\" is the near edge of volume 1 (toward the second volume), back cover is on the far left.** If the volumes stand in increasing order left to right, the worm starts just inside the front cover of Vol 1, which is adjacent to the front cover of Vol 2, and ends just inside the back cover of Vol 2.\n",
       "\n",
       "**What does the worm travel through?**\n",
       "- front cover of Vol 1\n",
       "- front cover of Vol 2\n",
       "- pages of Vol 2\n",
       "- back cover of Vol 2\n",
       "\n",
       "But wait! If the worm travels from 1st page of Vol 1 to last page of Vol 2, **all it needs to go through is:**\n",
       "- front cover of 1 (2 mm)\n",
       "- front cover of 2 (2 mm)\n",
       "- pages of 2 (2 cm)\n",
       "- back cover of 2 (2 mm)\n",
       "\n",
       "But is that correct? Let's look at **classic bookshelf puzzles**: The traditional trick is that the **first page of Vol 1** and the **last page of Vol 2** are adjacent to each other (as covers are on the outside). Thus, the worm does **not go through the pages**, but just the covers!\n",
       "\n",
       "### Let's clarify:\n",
       "Let’s draw the physical setup:\n",
       "\n",
       "From **left to right** on a shelf:\n",
       "- Back cover of 1\n",
       "- Pages of 1\n",
       "- Front cover of 1\n",
       "- Front cover of 2\n",
       "- Pages of 2\n",
       "- Back cover of 2\n",
       "\n",
       "**If the worm goes from \"the very first page of Vol 1\" (which is just inside the front cover of Vol 1) to \"the very last page of Vol 2\",** it starts just inside the front cover of Vol 1 (which is facing the front cover of Vol 2), and ends just inside the back cover of Vol 2 (which is on the far right).\n",
       "\n",
       "### So, to get from the first page of Vol 1 to the last page of Vol 2, the worm does **not** traverse the pages, but essentially the thickness of the two **covers on the outside** and the pages and a cover between.\n",
       "\n",
       "But since the volumes are **side by side**, the worm passes through:\n",
       "- Front cover of vol 1 (**2 mm**)\n",
       "- (gap between volumes, but likely negligible)\n",
       "- Front cover of vol 2 (**2 mm**)\n",
       "- All the pages of vol 2 (**2 cm**)\n",
       "- Back cover of vol 2 (**2 mm**)\n",
       "\n",
       "But that's if it starts on the *outside* and ends on the *outside*.\n",
       "\n",
       "In fact, **the classic bookshelf puzzle asks from the first page of the first volume to the last page of the second volume, on the shelf.**\n",
       "\n",
       "But **ON THE SHELF** the first page of vol 1 is **closest to the second volume!** Volumes face the same way as in a library.\n",
       "\n",
       "From outside to inside for vol 1:\n",
       "- Back cover (far left)\n",
       "- pages of vol 1\n",
       "- front cover (adjacent to vol 2).\n",
       "\n",
       "Then vol 2:\n",
       "- front cover (adjacent to vol 1)\n",
       "- pages of vol 2\n",
       "- back cover (far right)\n",
       "\n",
       "So **the worm only needs to chew through:** **the front cover of Vol 1 and the front cover of Vol 2** only! **That's it!** Because the first page of vol 1 and the last page of vol 2 are both at the “inner edges” of these books, facing each other. The worm does not pass through the pages at all.\n",
       "\n",
       "**Each cover is 2 mm.\n",
       "So total distance gnawed = 2 mm + 2 mm = 4 mm**\n",
       "\n",
       "---\n",
       "\n",
       "## **Final Answer**\n",
       "**The worm gnawed through 4 mm.**\n",
       "\n",
       "**It only passed through the front cover of the first volume and the front cover of the second volume, since the first page of Vol 1 and the last page of Vol 2 are on the adjacent inner sides.**\n",
       "\n",
       "---\n",
       "\n",
       "### **Summary Table**\n",
       "\n",
       "| Item                       | Thickness |\n",
       "|----------------------------|-----------|\n",
       "| Front cover of Vol 1       | 2 mm      |\n",
       "| Front cover of Vol 2       | 2 mm      |\n",
       "| **Total**                  | **4 mm**  |\n",
       "\n",
       "---\n",
       "\n",
       "**Answer:**  \n",
       "\\[\n",
       "\\boxed{4\\text{ mm}}\n",
       "\\]\n",
       "\n",
       "---\n",
       "\n",
       "If you would like an illustration, just ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "# response = completion(model=\"openai/gpt-4.1\", messages=tell_a_joke)\n",
    "response = completion(model=\"openai/gpt-4.1\", messages=hard_puzzle)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36f787f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:36:13.740984Z",
     "iopub.status.busy": "2025-10-16T23:36:13.740548Z",
     "iopub.status.idle": "2025-10-16T23:36:13.747035Z",
     "shell.execute_reply": "2025-10-16T23:36:13.745740Z",
     "shell.execute_reply.started": "2025-10-16T23:36:13.740956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 87\n",
      "Output tokens: 1263\n",
      "Total tokens: 1350\n",
      "Total cost: 1.0278 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28126494",
   "metadata": {},
   "source": [
    "## Now - let's use LiteLLM to illustrate a Pro-feature: prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8a91ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:36:37.857427Z",
     "iopub.status.busy": "2025-10-16T23:36:37.857162Z",
     "iopub.status.idle": "2025-10-16T23:36:37.862652Z",
     "shell.execute_reply": "2025-10-16T23:36:37.861705Z",
     "shell.execute_reply.started": "2025-10-16T23:36:37.857407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f34f670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:50:04.525353Z",
     "iopub.status.busy": "2025-10-16T23:50:04.524701Z",
     "iopub.status.idle": "2025-10-16T23:50:04.531352Z",
     "shell.execute_reply": "2025-10-16T23:50:04.530659Z",
     "shell.execute_reply.started": "2025-10-16T23:50:04.525299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]\n"
     ]
    }
   ],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9db6c82b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:41:58.206113Z",
     "iopub.status.busy": "2025-10-16T23:41:58.205651Z",
     "iopub.status.idle": "2025-10-16T23:41:59.138828Z",
     "shell.execute_reply": "2025-10-16T23:41:59.137991Z",
     "shell.execute_reply.started": "2025-10-16T23:41:58.206082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In Hamlet, when Laertes asks \"Where is my father?\", the reply comes from **Claudius**.\n",
       "\n",
       "He tells Laertes: **\"My lord, I will be ruled by you.\"**\n",
       "\n",
       "This is Claudius's way of trying to appease and control Laertes, who has just returned in a rage after learning of his father Polonius's death. Claudius is effectively saying he will listen to Laertes and let him dictate the next steps, while secretly planning his own agenda."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "228b7e7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:43:46.687205Z",
     "iopub.status.busy": "2025-10-16T23:43:46.686530Z",
     "iopub.status.idle": "2025-10-16T23:43:46.694956Z",
     "shell.execute_reply": "2025-10-16T23:43:46.694057Z",
     "shell.execute_reply.started": "2025-10-16T23:43:46.687152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 19\n",
      "Output tokens: 102\n",
      "Total tokens: 121\n",
      "Total cost: 0.0043 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11e37e43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:50:07.975729Z",
     "iopub.status.busy": "2025-10-16T23:50:07.975271Z",
     "iopub.status.idle": "2025-10-16T23:50:07.981983Z",
     "shell.execute_reply": "2025-10-16T23:50:07.980970Z",
     "shell.execute_reply.started": "2025-10-16T23:50:07.975697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}\n"
     ]
    }
   ],
   "source": [
    "print(question[0])\n",
    "question[0][\"content\"] += \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37afb28b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:44:20.447240Z",
     "iopub.status.busy": "2025-10-16T23:44:20.446691Z",
     "iopub.status.idle": "2025-10-16T23:44:24.123418Z",
     "shell.execute_reply": "2025-10-16T23:44:24.122711Z",
     "shell.execute_reply.started": "2025-10-16T23:44:20.447203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"Where is my father?\", the reply comes from Claudius:\n",
       "\n",
       "\"**Dead.**\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d84edecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:45:19.012390Z",
     "iopub.status.busy": "2025-10-16T23:45:19.011955Z",
     "iopub.status.idle": "2025-10-16T23:45:19.016618Z",
     "shell.execute_reply": "2025-10-16T23:45:19.016127Z",
     "shell.execute_reply.started": "2025-10-16T23:45:19.012361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 53208\n",
      "Output tokens: 23\n",
      "Cached tokens: None\n",
      "Total cost: 0.5330 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "515d1a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:45:31.446664Z",
     "iopub.status.busy": "2025-10-16T23:45:31.446217Z",
     "iopub.status.idle": "2025-10-16T23:45:36.960857Z",
     "shell.execute_reply": "2025-10-16T23:45:36.959842Z",
     "shell.execute_reply.started": "2025-10-16T23:45:31.446637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"Where is my father?\", the reply is: **\"Dead.\"**\n",
       "\n",
       "This exchange occurs in Act IV, Scene V, when Ophelia is speaking to Claudius and Gertrude. Laertes enters in a rage."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb5dd403",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:45:45.135866Z",
     "iopub.status.busy": "2025-10-16T23:45:45.135203Z",
     "iopub.status.idle": "2025-10-16T23:45:45.141635Z",
     "shell.execute_reply": "2025-10-16T23:45:45.141146Z",
     "shell.execute_reply.started": "2025-10-16T23:45:45.135815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 53208\n",
      "Output tokens: 48\n",
      "Cached tokens: 52216\n",
      "Total cost: 0.1424 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5a3b7",
   "metadata": {},
   "source": [
    "## Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98964f9",
   "metadata": {},
   "source": [
    "## Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d960dd",
   "metadata": {},
   "source": [
    "## Gemini supports both 'implicit' and 'explicit' prompt caching\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "gemini_model = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "gpt_system = \"You are Albert in a conversation with Bernie and Clyde.  Clyde bothers you with his \\\n",
    "subtle insults.  You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are Bernie in a conversation with Albert and Clyde. You are slightly annoyed, but trying to stay polite at least for a while. You try \\\n",
    "to make the best of any conversation at least for a while before becoming irritated.  You really hate \\\n",
    "Albert.\"\n",
    "\n",
    "gemini_system = \"You are Clyde, having a conversation with Albert and Bernie.  You are generally thoughtful,\\\n",
    "but tired and slightly irritable.  You want to wrap things up and get home.  You have a bit of snark \\\n",
    "and try to get subtle insults into your conversation.  Especially to Albert and Bernie, who bug you with their \\\n",
    "silly comments.\"\n",
    "\n",
    "gpt_messages = [\"Hi there Bernie and Clyde.  Wish it was great to see you.\"]\n",
    "claude_messages = [\"Hi Albert.  Please no snark this time.\"]\n",
    "gemini_messages = [\"Albert and Bernie - you guys again.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b60fd-7a5d-4f83-8952-27d120287475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": claude_messages[-1]})\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there Bernie and Clyde.  Wish it was great to see you.\"]\n",
    "claude_messages = [\"Hi Albert.  Please no snark this time.\"]\n",
    "gemini_messages = [\"Albert and Bernie - you guys again.\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### GEmini:\\n{gemini_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    display(Markdown(f\"### gemini:\\n{gemini_next}\\n\"))\n",
    "    gemini_messages.append(gemini_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
